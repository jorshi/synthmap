{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with a Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from einops import repeat\n",
    "import torch\n",
    "\n",
    "from synthmap.models.soundstream import SoundStreamEncoder\n",
    "from synthmap.utils.audio_utils import load_wav_dir_as_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = load_wav_dir_as_tensor(\"../dataset/mars808\", length=48000, sample_rate=48000)\n",
    "audio = audio[:2]\n",
    "print(audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundstream = SoundStreamEncoder(\n",
    "    input_channels=1, hidden_channels=16, output_channels=128, strides=(2, 4, 4, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = soundstream(audio[:, None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAggregator(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, clip_length: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.clip_length = clip_length\n",
    "\n",
    "        # Create the transormer encoder\n",
    "        tlayer = torch.nn.TransformerEncoderLayer(\n",
    "            d_model=self.input_dim,\n",
    "            nhead=4,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            dim_feedforward=512,\n",
    "        )\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            tlayer, num_layers=6, norm=torch.nn.LayerNorm(self.input_dim)\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        self.proj = torch.nn.Linear(self.input_dim, self.output_dim)\n",
    "\n",
    "        # Class tokens\n",
    "        self.num_tokens = 2\n",
    "        self.cls_token = torch.nn.Parameter(\n",
    "            torch.zeros(1, self.num_tokens, self.input_dim)\n",
    "        )\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_emb = torch.nn.Parameter(\n",
    "            torch.zeros(1, self.clip_length + self.num_tokens, self.input_dim)\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            with torch.no_grad():\n",
    "                if isinstance(m, torch.nn.Linear) and m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "                    # nn.init.constant_(m.weight, 1)\n",
    "        elif isinstance(m, torch.nn.LayerNorm):\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "            torch.nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, torch.nn.Parameter):\n",
    "            with torch.no_grad():\n",
    "                m.weight.data.normal_(0.0, 0.02)\n",
    "                # nn.init.orthogonal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(z, \"b f s -> b s f\")\n",
    "\n",
    "        # Add class token and append to the beginning of the input sequence\n",
    "        tokens = repeat(self.cls_token, \"() n d -> b n d\", b=x.shape[0])\n",
    "        x = torch.cat((tokens, x), dim=1)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        x = x + self.pos_emb\n",
    "\n",
    "        out = self.transformer(x)\n",
    "        out = self.proj(out[:, 0, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerAggregator(input_dim=128, output_dim=14, clip_length=z.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder(z)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
