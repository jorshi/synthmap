{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "from einops import repeat\n",
    "from einops import rearrange\n",
    "\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "import torchaudio\n",
    "from hear_ced import ced_base\n",
    "\n",
    "from evotorch import Problem\n",
    "from evotorch.algorithms import SteadyStateGA\n",
    "from evotorch.operators import (\n",
    "    SimulatedBinaryCrossOver,\n",
    "    GaussianMutation,\n",
    ")\n",
    "from evotorch.logging import StdOutLogger\n",
    "\n",
    "from synthmap.synth import Snare808\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_path = Path(\"../audio/snares\")\n",
    "audio_files = list(background_path.rglob(\"*.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ced_base.load_model(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = []\n",
    "for file in audio_files:\n",
    "    waveform, sample_rate = torchaudio.load(file)\n",
    "\n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform[:1]\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sample_rate != model.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(\n",
    "            waveform, sample_rate, model.sample_rate, lowpass_filter_width=512\n",
    "        )\n",
    "\n",
    "    # Pad to minimum length\n",
    "    if waveform.shape[-1] < model.sample_rate:\n",
    "        waveform = torch.nn.functional.pad(\n",
    "            waveform, (0, model.sample_rate - waveform.shape[-1])\n",
    "        )\n",
    "\n",
    "    audio.append(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for waveform in audio:\n",
    "    emb = ced_base.get_scene_embeddings(waveform.to(device), model)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = torch.vstack(embeddings)\n",
    "\n",
    "mean = torch.mean(embeddings, dim=0)\n",
    "cov = torch.cov(embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.functional.frechet_distance(mean, cov, mean, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snare = Snare808(48000, 48000)\n",
    "\n",
    "num_params = snare.get_num_params()\n",
    "params = torch.rand(1, num_params)\n",
    "\n",
    "y = snare(params)\n",
    "\n",
    "ipd.display(ipd.Audio(y, rate=48000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=snare.sample_rate,\n",
    "    n_fft=2048,\n",
    "    hop_length=128,\n",
    "    n_mels=128,\n",
    ")\n",
    "mel_spectrogram = mel_spectrogram.to(device)\n",
    "\n",
    "mel_target = []\n",
    "\n",
    "for file in audio_files:\n",
    "    waveform, sample_rate = torchaudio.load(file)\n",
    "\n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform[:1]\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sample_rate != model.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(\n",
    "            waveform, sample_rate, snare.sample_rate, lowpass_filter_width=512\n",
    "        )\n",
    "\n",
    "    # Ensure 1sec length\n",
    "    if waveform.shape[-1] < snare.sample_rate:\n",
    "        waveform = torch.nn.functional.pad(\n",
    "            waveform, (0, snare.sample_rate - waveform.shape[-1])\n",
    "        )\n",
    "    elif waveform.shape[-1] > snare.sample_rate:\n",
    "        waveform = waveform[:, : snare.sample_rate]\n",
    "\n",
    "    # Normalize\n",
    "    waveform = waveform / waveform.abs().max()\n",
    "\n",
    "    mel = mel_spectrogram(waveform.to(device))\n",
    "    mel_target.append(mel)\n",
    "\n",
    "mel_target = torch.vstack(mel_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_minmax(audio: torch.Tensor, targets: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Given a batch of audio and a batch of targets, find the minimum and maximum\n",
    "    errors between the audio and the target.\n",
    "    \"\"\"\n",
    "    minimums = torch.zeros(audio.shape[0], device=audio.device)\n",
    "    maximums = torch.zeros(audio.shape[0], device=audio.device)\n",
    "    error = torch.zeros(audio.shape[0], device=audio.device)\n",
    "    for i in range(audio.shape[0]):\n",
    "        diff = torch.mean(torch.abs(audio[i] - targets), dim=(-1, -2))\n",
    "        minimums[i] = torch.min(diff)\n",
    "        maximums[i] = torch.max(diff)\n",
    "        error[i] = torch.mean(diff)\n",
    "\n",
    "    # Iterate through the audio patch and select the best and worst matches\n",
    "    return minimums, maximums, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, **mel_kwargs):\n",
    "        super().__init__()\n",
    "        self.mel_spectrogram = torchaudio.transforms.MelSpectrogram(**mel_kwargs)\n",
    "\n",
    "    def forward(self, audio: torch.Tensor):\n",
    "        x = self.mel_spectrogram(audio)\n",
    "\n",
    "        # Summarize\n",
    "        x_mean = torch.mean(x, dim=(-1))\n",
    "        x_diff = torch.mean(torch.diff(x, dim=-1), dim=-1)\n",
    "\n",
    "        return torch.hstack([x_mean, x_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_embed = MelEmbedding(\n",
    "    sample_rate=snare.sample_rate, n_fft=2048, hop_length=128, n_mels=128\n",
    ")\n",
    "mel_embed = mel_embed.to(device)\n",
    "\n",
    "embed = mel_embed(y.to(device))\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_embed_target = []\n",
    "for file in audio_files:\n",
    "    waveform, sample_rate = torchaudio.load(file)\n",
    "\n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform[:1]\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sample_rate != model.sample_rate:\n",
    "        waveform = torchaudio.functional.resample(\n",
    "            waveform, sample_rate, snare.sample_rate, lowpass_filter_width=512\n",
    "        )\n",
    "\n",
    "    # Ensure 1sec length\n",
    "    if waveform.shape[-1] < snare.sample_rate:\n",
    "        waveform = torch.nn.functional.pad(\n",
    "            waveform, (0, snare.sample_rate - waveform.shape[-1])\n",
    "        )\n",
    "    elif waveform.shape[-1] > snare.sample_rate:\n",
    "        waveform = waveform[:, : snare.sample_rate]\n",
    "\n",
    "    # Normalize\n",
    "    waveform = waveform / waveform.abs().max()\n",
    "    mel_embed_target.append(mel_embed(waveform.to(device)))\n",
    "\n",
    "mel_embed_target = torch.vstack(mel_embed_target)\n",
    "print(mel_embed_target.shape)\n",
    "\n",
    "mel_mean = torch.mean(mel_embed_target, dim=0)\n",
    "mel_cov = torch.cov(mel_embed_target.T)\n",
    "\n",
    "print(mel_mean.shape, mel_cov.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EvoTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_synth_distance(params: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    # Generate audio\n",
    "    y = snare(torch.clamp(params, 0.0, 1.0))\n",
    "\n",
    "    # Resample to CED sample rate\n",
    "    y_down = torchaudio.functional.resample(\n",
    "        y, 48000, model.sample_rate, lowpass_filter_width=512\n",
    "    )\n",
    "\n",
    "    # Compute embeddings\n",
    "    with torch.no_grad():\n",
    "        y_down = y_down.clone()\n",
    "        emb = model.clip_embedding(y_down)\n",
    "\n",
    "    # Split into chunks and compute Frechet distance for each chunk\n",
    "    emb_chunk = torch.chunk(emb, 8, dim=0)\n",
    "    distances = []\n",
    "    for chunk in emb_chunk:\n",
    "        emb_mean = chunk.mean(dim=0)\n",
    "        emb_cov = torch.cov(chunk.T)\n",
    "\n",
    "        dist = torchaudio.functional.frechet_distance(mean, cov, emb_mean, emb_cov)\n",
    "        dist = repeat(dist.unsqueeze(0), \"() -> b\", b=chunk.shape[0])\n",
    "        distances.append(dist)\n",
    "\n",
    "    dist = torch.hstack(distances)\n",
    "\n",
    "    # Compute frechet on the mel embeddings\n",
    "    # with torch.no_grad():\n",
    "    #     emb = mel_embed(y)\n",
    "\n",
    "    # emb_chunk = torch.chunk(emb, 50, dim=0)\n",
    "    # distances = []\n",
    "    # for chunk in emb_chunk:\n",
    "    #     emb_mean = chunk.mean(dim=0)\n",
    "    #     emb_cov = torch.cov(chunk.T)\n",
    "\n",
    "    #     mel_dist = torchaudio.functional.frechet_distance(mel_mean, mel_cov, emb_mean, emb_cov)\n",
    "    #     mel_dist = repeat(mel_dist.unsqueeze(0), '() -> b', b=chunk.shape[0])\n",
    "    #     distances.append(mel_dist)\n",
    "\n",
    "    # mel_dist = torch.hstack(distances)\n",
    "\n",
    "    # Compute the minimum and maximum distance after normalizing\n",
    "    max_sample = torch.max(torch.abs(y), dim=-1).values\n",
    "    y = y / max_sample[:, None]\n",
    "    mel_audio = mel_spectrogram(y)\n",
    "\n",
    "    min, max, error = mel_minmax(mel_audio, mel_target)\n",
    "\n",
    "    # # Minimize the error to the sample with the maximum distance\n",
    "    # # Could potentially do this in chunks as well.\n",
    "    # mel_chunk = torch.chunk(mel_audio, 2, dim=0)\n",
    "    # mel_distances = []\n",
    "    # for chunk in mel_chunk:\n",
    "    #     _, max, _ = mel_minmax(mel_target, mel_audio)\n",
    "    #     max = torch.max(max)\n",
    "    #     max = repeat(max.unsqueeze(0), '() -> b', b=chunk.shape[0])\n",
    "    #     mel_distances.append(max)\n",
    "\n",
    "    # max = torch.hstack(mel_distances)\n",
    "\n",
    "    # mel_audio = torch.mean(mel_audio, dim=-1)\n",
    "    # mel_dist = torch.cdist(mel_audio, mel_audio, p=2)\n",
    "    # mel_dist = torch.mean(mel_dist, dim=-1)\n",
    "    # mel_dist = mel_dist * 0.0001\n",
    "\n",
    "    fitness = torch.stack([dist, max], dim=-1)\n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = Problem(\n",
    "    # Three objectives\n",
    "    [\"min\", \"min\"],\n",
    "    compute_synth_distance,\n",
    "    initial_bounds=(0.0, 1.0),\n",
    "    bounds=(0.0, 1.0),\n",
    "    solution_length=num_params,\n",
    "    vectorized=True,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "ga = SteadyStateGA(prob, popsize=200)\n",
    "ga.use(\n",
    "    SimulatedBinaryCrossOver(\n",
    "        prob,\n",
    "        tournament_size=4,\n",
    "        cross_over_rate=1.0,\n",
    "        eta=8,\n",
    "    )\n",
    ")\n",
    "ga.use(GaussianMutation(prob, stdev=0.3))\n",
    "\n",
    "logger = StdOutLogger(ga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = ga.population.values.clone()\n",
    "# y = snare(torch.clamp(params, 0.0 ,1.0))\n",
    "\n",
    "# # Normalize each sample\n",
    "# y_max = torch.max(torch.abs(y), dim=1).values\n",
    "# y = y / y_max[:, None]\n",
    "\n",
    "# y = rearrange(y, 'b n -> 1 (b n)')\n",
    "# ipd.display(ipd.Audio(y.detach().cpu(), rate=48000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in ga.population[:50]:\n",
    "    print(p.evals)\n",
    "    y = snare(torch.clamp(p.values, 0.0, 1.0)[None])\n",
    "    ipd.display(ipd.Audio(y.detach().cpu(), rate=48000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
